{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f99c7213",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcffcbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5efc823c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.2000, 0.3000],\n",
       "        [0.4000, 0.5000, 0.6000],\n",
       "        [0.7000, 0.8000, 0.9000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.FloatTensor([[.1, .2, .3],\n",
    "                            [.4, .5, .6],\n",
    "                            [.7, .8, .9]])\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33c70715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5425, 0.6250, 0.6944],\n",
       "        [0.2591, 0.2604, 0.1496],\n",
       "        [0.1317, 0.5961, 0.9846]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand_like(target).requires_grad_(True)\n",
    "\n",
    "## This means the final scalar will be differentiate by x.\n",
    "## You can get gradient of x, after differentiation.\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f468079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1315, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.mse_loss(x, target)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6da28ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th Loss: 7.9569e-02\n",
      "tensor([[0.4442, 0.5305, 0.6068],\n",
      "        [0.2904, 0.3136, 0.2497],\n",
      "        [0.2580, 0.6414, 0.9658]], requires_grad=True)\n",
      "2-th Loss: 4.8134e-02\n",
      "tensor([[0.3677, 0.4571, 0.5386],\n",
      "        [0.3148, 0.3550, 0.3276],\n",
      "        [0.3562, 0.6767, 0.9512]], requires_grad=True)\n",
      "3-th Loss: 2.9118e-02\n",
      "tensor([[0.3082, 0.3999, 0.4856],\n",
      "        [0.3337, 0.3872, 0.3881],\n",
      "        [0.4326, 0.7041, 0.9398]], requires_grad=True)\n",
      "4-th Loss: 1.7615e-02\n",
      "tensor([[0.2619, 0.3555, 0.4443],\n",
      "        [0.3484, 0.4123, 0.4352],\n",
      "        [0.4920, 0.7254, 0.9309]], requires_grad=True)\n",
      "5-th Loss: 1.0656e-02\n",
      "tensor([[0.2260, 0.3210, 0.4123],\n",
      "        [0.3599, 0.4318, 0.4718],\n",
      "        [0.5382, 0.7420, 0.9241]], requires_grad=True)\n",
      "6-th Loss: 6.4462e-03\n",
      "tensor([[0.1980, 0.2941, 0.3873],\n",
      "        [0.3688, 0.4469, 0.5003],\n",
      "        [0.5742, 0.7549, 0.9187]], requires_grad=True)\n",
      "7-th Loss: 3.8995e-03\n",
      "tensor([[0.1762, 0.2732, 0.3679],\n",
      "        [0.3757, 0.4587, 0.5225],\n",
      "        [0.6021, 0.7649, 0.9146]], requires_grad=True)\n",
      "8-th Loss: 2.3590e-03\n",
      "tensor([[0.1593, 0.2569, 0.3528],\n",
      "        [0.3811, 0.4679, 0.5397],\n",
      "        [0.6239, 0.7727, 0.9113]], requires_grad=True)\n",
      "9-th Loss: 1.4270e-03\n",
      "tensor([[0.1461, 0.2443, 0.3411],\n",
      "        [0.3853, 0.4750, 0.5531],\n",
      "        [0.6408, 0.7788, 0.9088]], requires_grad=True)\n",
      "10-th Loss: 8.6327e-04\n",
      "tensor([[0.1359, 0.2344, 0.3320],\n",
      "        [0.3886, 0.4806, 0.5635],\n",
      "        [0.6540, 0.7835, 0.9069]], requires_grad=True)\n",
      "11-th Loss: 5.2222e-04\n",
      "tensor([[0.1279, 0.2268, 0.3249],\n",
      "        [0.3911, 0.4849, 0.5716],\n",
      "        [0.6642, 0.7872, 0.9053]], requires_grad=True)\n",
      "12-th Loss: 3.1591e-04\n",
      "tensor([[0.1217, 0.2208, 0.3193],\n",
      "        [0.3931, 0.4883, 0.5779],\n",
      "        [0.6721, 0.7900, 0.9041]], requires_grad=True)\n",
      "13-th Loss: 1.9111e-04\n",
      "tensor([[0.1169, 0.2162, 0.3150],\n",
      "        [0.3946, 0.4909, 0.5828],\n",
      "        [0.6783, 0.7922, 0.9032]], requires_grad=True)\n",
      "14-th Loss: 1.1561e-04\n",
      "tensor([[0.1131, 0.2126, 0.3117],\n",
      "        [0.3958, 0.4929, 0.5866],\n",
      "        [0.6832, 0.7940, 0.9025]], requires_grad=True)\n",
      "15-th Loss: 6.9936e-05\n",
      "tensor([[0.1102, 0.2098, 0.3091],\n",
      "        [0.3968, 0.4945, 0.5896],\n",
      "        [0.6869, 0.7953, 0.9020]], requires_grad=True)\n",
      "16-th Loss: 4.2307e-05\n",
      "tensor([[0.1079, 0.2076, 0.3071],\n",
      "        [0.3975, 0.4957, 0.5919],\n",
      "        [0.6898, 0.7963, 0.9015]], requires_grad=True)\n",
      "17-th Loss: 2.5593e-05\n",
      "tensor([[0.1062, 0.2059, 0.3055],\n",
      "        [0.3980, 0.4967, 0.5937],\n",
      "        [0.6921, 0.7972, 0.9012]], requires_grad=True)\n",
      "18-th Loss: 1.5482e-05\n",
      "tensor([[0.1048, 0.2046, 0.3043],\n",
      "        [0.3985, 0.4974, 0.5951],\n",
      "        [0.6938, 0.7978, 0.9009]], requires_grad=True)\n",
      "19-th Loss: 9.3657e-06\n",
      "tensor([[0.1037, 0.2036, 0.3033],\n",
      "        [0.3988, 0.4980, 0.5962],\n",
      "        [0.6952, 0.7983, 0.9007]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-5\n",
    "learning_rate = 1.\n",
    "iter_cnt = 0\n",
    "\n",
    "while loss > threshold:\n",
    "    iter_cnt += 1\n",
    "    \n",
    "    loss.backward() ## Calculate gradients.\n",
    "    \n",
    "    x = x - learning_rate * x.grad\n",
    "    \n",
    "    # You don't need to aware this now.\n",
    "    x.detach_()\n",
    "    x.requires_grad_(True)\n",
    "    \n",
    "    loss = F.mse_loss(x, target)\n",
    "    \n",
    "    print('%d-th Loss: %.4e' % (iter_cnt, loss))\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8a58a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
